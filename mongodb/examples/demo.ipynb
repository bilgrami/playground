{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# JSON Flattening Toolkit - Comprehensive Guide for Data Engineers & Scientists\n",
        "\n",
        "> **A world-class exploration of JSON flattening techniques, patterns, and real-world applications**\n",
        "\n",
        "This notebook is organized into **10 self-contained milestones**, each focusing on specific aspects of JSON flattening. You can work through them sequentially or jump to specific topics of interest.\n",
        "\n",
        "## üìö Table of Contents\n",
        "\n",
        "### Foundations\n",
        "- **[Milestone 1: Foundations & Core Concepts](#milestone-1)** - Basic flattening, list policies, separators\n",
        "- **[Milestone 2: Array Handling Strategies](#milestone-2)** - Index vs join, explosion, cartesian products\n",
        "\n",
        "### Advanced Techniques  \n",
        "- **[Milestone 3: Complex Structures](#milestone-3)** - Deep nesting, mixed types, null handling\n",
        "\n",
        "### Real-World Use Cases\n",
        "- **[Milestone 4: E-commerce Data](#milestone-4)** - Orders, products, customers, transactions\n",
        "- **[Milestone 5: API & Event Data](#milestone-5)** - API responses, webhooks, event logs\n",
        "\n",
        "### Data Pipelines\n",
        "- **[Milestone 6: CSV Operations & Pipelines](#milestone-6)** - Read/write, transformations, batch processing\n",
        "\n",
        "### Database Integration\n",
        "- **[Milestone 7: MongoDB Integration](#milestone-7)** - Ingestion, querying, type inference\n",
        "- **[Milestone 8: Snowflake Integration](#milestone-8)** - Schema generation, ingestion, queries\n",
        "\n",
        "### Production Patterns\n",
        "- **[Milestone 9: Advanced Patterns & Best Practices](#milestone-9)** - Performance, memory, error handling\n",
        "- **[Milestone 10: End-to-End Workflows](#milestone-10)** - Complete pipelines, production examples\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will be able to:\n",
        "- ‚úÖ Flatten complex nested JSON structures efficiently\n",
        "- ‚úÖ Choose appropriate array handling strategies for your use case\n",
        "- ‚úÖ Build data pipelines from JSON to CSV to databases\n",
        "- ‚úÖ Handle edge cases (nulls, empty arrays, mixed types)\n",
        "- ‚úÖ Integrate with MongoDB and Snowflake\n",
        "- ‚úÖ Apply best practices for production systems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Quick Start\n",
        "\n",
        "Let's set up our environment and import the necessary modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# IMPORTS - All imports at the top for clarity\n",
        "# ============================================================================\n",
        "\n",
        "import json\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import Any, Dict, List, Optional\n",
        "from collections import Counter\n",
        "\n",
        "# Ensure local package is importable when running in Docker/Repo root\n",
        "PROJECT_ROOT = Path.cwd()\n",
        "if (PROJECT_ROOT / \"json_flatten\").exists():\n",
        "    sys.path.insert(0, str(PROJECT_ROOT))\n",
        "elif (PROJECT_ROOT / \"mongodb\" / \"json_flatten\").exists():\n",
        "    sys.path.insert(0, str(PROJECT_ROOT / \"mongodb\"))\n",
        "\n",
        "# Core flattening functions\n",
        "from json_flatten import flatten_json, flatten_records, write_csv, read_csv\n",
        "from json_flatten.scenarios import get_scenarios\n",
        "\n",
        "# Optional: MongoDB and Snowflake (may not be available)\n",
        "try:\n",
        "    from json_flatten.mongodb_io import ingest_csv_to_mongodb, query_mongodb, infer_type\n",
        "    MONGO_AVAILABLE = True\n",
        "except ImportError:\n",
        "    MONGO_AVAILABLE = False\n",
        "    print(\"‚ö† MongoDB integration not available (pymongo not installed)\")\n",
        "\n",
        "try:\n",
        "    from json_flatten.snowflake_io import create_table_schema, ingest_csv_to_snowflake, query_snowflake\n",
        "    SNOWFLAKE_AVAILABLE = True\n",
        "except ImportError:\n",
        "    SNOWFLAKE_AVAILABLE = False\n",
        "    print(\"‚ö† Snowflake integration not available (snowflake-connector-python not installed)\")\n",
        "\n",
        "# PySpark imports (for large-scale processing)\n",
        "try:\n",
        "    import findspark\n",
        "    findspark.init()\n",
        "    from pyspark.sql import SparkSession\n",
        "    from pyspark.sql.functions import col, explode, from_json, schema_of_json\n",
        "    from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, BooleanType, ArrayType\n",
        "    PYSPARK_AVAILABLE = True\n",
        "except ImportError:\n",
        "    PYSPARK_AVAILABLE = False\n",
        "    print(\"‚ö† PySpark not available (pyspark not installed)\")\n",
        "\n",
        "# Setup output directory\n",
        "OUTPUT_DIR = Path(\"notebook_output\")\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Helper functions for pretty printing and analysis\n",
        "def print_section(title: str, char: str = \"=\"):\n",
        "    \"\"\"Print a formatted section header.\"\"\"\n",
        "    print(f\"\\n{char * 60}\")\n",
        "    print(f\"  {title}\")\n",
        "    print(f\"{char * 60}\\n\")\n",
        "\n",
        "def compare_before_after(before: Any, after: Dict[str, Any], title: str = \"Transformation\"):\n",
        "    \"\"\"Compare original and flattened data side by side.\"\"\"\n",
        "    print_section(title)\n",
        "    print(\"BEFORE (Original JSON):\")\n",
        "    print(json.dumps(before, indent=2))\n",
        "    print(\"\\nAFTER (Flattened):\")\n",
        "    print(json.dumps(after, indent=2))\n",
        "    print(f\"\\nüìä Flattened to {len(after)} fields\")\n",
        "\n",
        "def measure_time(func):\n",
        "    \"\"\"Decorator to measure execution time.\"\"\"\n",
        "    def wrapper(*args, **kwargs):\n",
        "        start = time.time()\n",
        "        result = func(*args, **kwargs)\n",
        "        elapsed = time.time() - start\n",
        "        print(f\"‚è±Ô∏è  Execution time: {elapsed:.4f} seconds\")\n",
        "        return result\n",
        "    return wrapper\n",
        "\n",
        "# Initialize PySpark if available\n",
        "if PYSPARK_AVAILABLE:\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"JSONFlattening\") \\\n",
        "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "        .getOrCreate()\n",
        "    spark.sparkContext.setLogLevel(\"WARN\")  # Reduce verbosity\n",
        "    print(\"‚úÖ PySpark session initialized\")\n",
        "\n",
        "print(\"‚úÖ Environment setup complete!\")\n",
        "print(f\"üìÅ Output directory: {OUTPUT_DIR.absolute()}\")\n",
        "print(f\"üîß MongoDB available: {MONGO_AVAILABLE}\")\n",
        "print(f\"‚ùÑÔ∏è  Snowflake available: {SNOWFLAKE_AVAILABLE}\")\n",
        "print(f\"‚ö° PySpark available: {PYSPARK_AVAILABLE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a id=\"milestone-1\"></a>\n",
        "\n",
        "# Milestone 1: Foundations & Core Concepts\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand the fundamental concept of JSON flattening\n",
        "- Learn how nested structures are converted to flat dictionaries\n",
        "- Explore different list handling policies\n",
        "- Master custom separator usage\n",
        "\n",
        "## Why Flatten JSON?\n",
        "\n",
        "Data engineers and data scientists frequently encounter challenges:\n",
        "- **Tabular formats** (CSV, databases) require flat structures\n",
        "- **Analytics tools** work better with normalized data\n",
        "- **Schema inference** is easier with flat structures\n",
        "- **Database ingestion** requires consistent column structures\n",
        "\n",
        "Let's start with the basics!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### 1.1 Understanding Nested Structures\n",
        "\n",
        "**What is nesting?**  \n",
        "Nesting occurs when JSON objects contain other objects or arrays inside them. Think of it like Russian dolls - objects within objects.\n",
        "\n",
        "**Why is this a problem?**  \n",
        "- Databases expect flat tables with columns\n",
        "- CSV files are inherently flat (rows and columns)\n",
        "- Analytics tools work better with normalized data\n",
        "- Schema inference becomes complex with nested structures\n",
        "\n",
        "**How does flattening work?**  \n",
        "The `flatten_json()` function recursively traverses nested structures and creates dot-delimited keys. For example:\n",
        "- `user.profile.name` represents the `name` field inside `profile` inside `user`\n",
        "- The dot (`.`) is the default separator, but you can customize it\n",
        "\n",
        "Let's see this in action:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 1: Simple nested structure\n",
        "data1 = {\n",
        "    \"user\": {\n",
        "        \"id\": 42,\n",
        "        \"profile\": {\n",
        "            \"name\": \"Alice\",\n",
        "            \"active\": True\n",
        "        }\n",
        "    },\n",
        "    \"score\": 9.5\n",
        "}\n",
        "\n",
        "flattened1 = flatten_json(data1)\n",
        "compare_before_after(data1, flattened1, \"Example 1: Simple Nested Structure\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Custom Separators\n",
        "\n",
        "**Why use custom separators?**  \n",
        "Sometimes the default dot (`.`) separator can conflict with your data:\n",
        "- Field names might contain dots\n",
        "- You might prefer underscores (`_`) or double underscores (`__`)\n",
        "- Some systems have naming conventions\n",
        "\n",
        "**Example use cases:**\n",
        "- MongoDB uses dots for nested queries, so you might want `_` instead\n",
        "- Some databases prefer `__` for clarity\n",
        "- Your organization might have specific naming standards\n",
        "\n",
        "Let's explore different separators:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Array Handling\n",
        "\n",
        "Arrays can be handled in two ways:\n",
        "- **Index policy**: Creates indexed keys (e.g., `tags.0`, `tags.1`)\n",
        "- **Join policy**: Joins primitive arrays with commas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Array explosion - creating multiple records\n",
        "data5 = {\n",
        "    \"order_id\": 1001,\n",
        "    \"customer\": \"Alice\",\n",
        "    \"items\": [\n",
        "        {\"sku\": \"A1\", \"qty\": 2, \"price\": 10.50},\n",
        "        {\"sku\": \"B2\", \"qty\": 1, \"price\": 5.25},\n",
        "        {\"sku\": \"C3\", \"qty\": 3, \"price\": 8.00}\n",
        "    ]\n",
        "}\n",
        "\n",
        "records = flatten_records(data5, explode_paths=[\"items\"])\n",
        "print(f\"Created {len(records)} records from array explosion:\")\n",
        "for i, record in enumerate(records, 1):\n",
        "    print(f\"\\nRecord {i}:\")\n",
        "    print(json.dumps(record, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CSV Operations\n",
        "\n",
        "Converting flattened JSON to CSV format for database ingestion or analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create output directory\n",
        "output_dir = Path(\"notebook_output\")\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Flatten and write to CSV\n",
        "sample_data = {\n",
        "    \"order_id\": 1001,\n",
        "    \"customer\": {\"name\": \"Alice\", \"email\": \"alice@example.com\"},\n",
        "    \"items\": [\n",
        "        {\"sku\": \"A1\", \"qty\": 2},\n",
        "        {\"sku\": \"B2\", \"qty\": 1}\n",
        "    ]\n",
        "}\n",
        "\n",
        "records = flatten_records(sample_data, explode_paths=[\"items\"])\n",
        "csv_path = output_dir / \"orders.csv\"\n",
        "write_csv(records, csv_path)\n",
        "\n",
        "print(f\"‚úì Written {len(records)} records to {csv_path}\")\n",
        "print(\"\\nCSV content:\")\n",
        "print(csv_path.read_text())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Milestone 1 Summary\n",
        "\n",
        "This toolkit provides comprehensive solutions for:\n",
        "1. **Flattening complex JSON structures** with configurable policies\n",
        "2. **Handling arrays** through indexing or explosion\n",
        "3. **Creating cartesian products** from multiple array paths\n",
        "4. **CSV conversion** for tabular data formats\n",
        "5. **Database ingestion** into MongoDB and Snowflake\n",
        "\n",
        "See README.md for complete documentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a id=\"milestone-2\"></a>\n",
        "\n",
        "# Milestone 2: Array Handling Strategies\n",
        "\n",
        "## Learning Objectives\n",
        "- Compare index vs join list policies\n",
        "- Understand array explosion into multiple records\n",
        "- Create cartesian products across multiple array paths\n",
        "\n",
        "Arrays are where flattening decisions have the biggest downstream impact. We'll compare policies and then explode arrays into multiple records."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_section(\"Index vs Join list policies\")\n",
        "\n",
        "array_data = {\n",
        "    \"tags\": [\"alpha\", \"beta\", \"gamma\"],\n",
        "    \"metrics\": {\"scores\": [10, 20, None]},\n",
        "    \"meta\": {\"ids\": [1, 2, 3]},\n",
        "}\n",
        "\n",
        "flatten_index = flatten_json(array_data, list_policy=\"index\")\n",
        "flatten_join = flatten_json(array_data, list_policy=\"join\")\n",
        "\n",
        "print(\"Index policy output:\")\n",
        "print(json.dumps(flatten_index, indent=2))\n",
        "print(\"\\nJoin policy output:\")\n",
        "print(json.dumps(flatten_join, indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_scenario_by_name(name: str):\n",
        "    scenarios = {scenario.name: scenario for scenario in get_scenarios()}\n",
        "    if name not in scenarios:\n",
        "        raise KeyError(f\"Scenario {name!r} not found\")\n",
        "    return scenarios[name]\n",
        "\n",
        "\n",
        "def run_scenario(name: str, max_records: int = 3):\n",
        "    scenario = get_scenario_by_name(name)\n",
        "    print_section(f\"Scenario: {scenario.name}\")\n",
        "    print(scenario.description)\n",
        "\n",
        "    if scenario.mode == \"records\":\n",
        "        records = flatten_records(\n",
        "            scenario.data,\n",
        "            explode_paths=scenario.explode_paths,\n",
        "            list_policy=scenario.list_policy,\n",
        "        )\n",
        "        print(f\"Records: {len(records)}\")\n",
        "        for record in records[:max_records]:\n",
        "            print(json.dumps(record, indent=2))\n",
        "        if len(records) > max_records:\n",
        "            print(f\"... {len(records) - max_records} more\")\n",
        "        return records\n",
        "\n",
        "    flattened = flatten_json(scenario.data, list_policy=scenario.list_policy)\n",
        "    print(json.dumps(flattened, indent=2))\n",
        "    return flattened"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_section(\"Array explosion and cartesian products\")\n",
        "\n",
        "scenario = get_scenario_by_name(\"multi_path_explosion\")\n",
        "records = flatten_records(\n",
        "    scenario.data,\n",
        "    explode_paths=scenario.explode_paths,\n",
        "    list_policy=scenario.list_policy,\n",
        ")\n",
        "\n",
        "print(f\"Exploded to {len(records)} records\")\n",
        "for record in records:\n",
        "    print(json.dumps(record, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a id=\"milestone-3\"></a>\n",
        "\n",
        "# Milestone 3: Complex Structures\n",
        "\n",
        "## Learning Objectives\n",
        "- Handle deep nesting and mixed types\n",
        "- Process nulls, empty arrays, and optional fields\n",
        "- Work with nested arrays inside arrays\n",
        "\n",
        "These scenarios mirror real data engineering edge cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "run_scenario(\"deep_nesting\")\n",
        "run_scenario(\"mixed_types\")\n",
        "run_scenario(\"empty_and_null_handling\")\n",
        "run_scenario(\"nested_arrays\", max_records=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a id=\"milestone-4\"></a>\n",
        "\n",
        "# Milestone 4: E-commerce Data\n",
        "\n",
        "## Learning Objectives\n",
        "- Flatten orders with line items\n",
        "- Create cartesian combinations across items and discounts\n",
        "- Preserve customer metadata\n",
        "\n",
        "We'll use the built-in `data/orders.json` and enrich it with customer fields."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "orders_path = Path(\"data/orders.json\")\n",
        "orders = json.loads(orders_path.read_text())\n",
        "\n",
        "orders[\"customer\"] = {\n",
        "    \"id\": \"cust_001\",\n",
        "    \"name\": \"Ada Lovelace\",\n",
        "    \"segment\": \"enterprise\",\n",
        "}\n",
        "\n",
        "records = flatten_records(orders, explode_paths=[\"items\", \"discounts\"])\n",
        "print(f\"Created {len(records)} order records\")\n",
        "for record in records:\n",
        "    print(json.dumps(record, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a id=\"milestone-5\"></a>\n",
        "\n",
        "# Milestone 5: API & Event Data\n",
        "\n",
        "## Learning Objectives\n",
        "- Flatten nested API responses\n",
        "- Handle event log arrays\n",
        "- Normalize timestamps for analytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "api_response = {\n",
        "    \"request_id\": \"req_123\",\n",
        "    \"status\": \"ok\",\n",
        "    \"data\": {\n",
        "        \"user\": {\"id\": 7, \"name\": \"Grace\"},\n",
        "        \"roles\": [\"admin\", \"editor\"],\n",
        "        \"metadata\": {\"source\": \"web\", \"region\": \"us-east-1\"},\n",
        "    },\n",
        "}\n",
        "\n",
        "flattened_api = flatten_json(api_response, list_policy=\"join\")\n",
        "compare_before_after(api_response, flattened_api, \"API Response Flattening\")\n",
        "\n",
        "print_section(\"Event log normalization\")\n",
        "event_payload = {\n",
        "    \"service\": \"billing\",\n",
        "    \"events\": [\n",
        "        {\"type\": \"created\", \"timestamp\": \"2024-01-15T10:30:00Z\", \"amount\": 45.5},\n",
        "        {\"type\": \"captured\", \"timestamp\": \"2024-01-15T10:31:05Z\", \"amount\": 45.5},\n",
        "    ],\n",
        "}\n",
        "\n",
        "records = flatten_records(event_payload, explode_paths=[\"events\"])\n",
        "print(f\"Created {len(records)} event records\")\n",
        "for record in records:\n",
        "    print(json.dumps(record, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a id=\"milestone-6\"></a>\n",
        "\n",
        "# Milestone 6: CSV Operations & Pipelines\n",
        "\n",
        "## Learning Objectives\n",
        "- Write flattened records to CSV\n",
        "- Read CSV back into Python\n",
        "- Build repeatable batch pipelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scenario = get_scenario_by_name(\"list_of_objects_explode\")\n",
        "records = flatten_records(\n",
        "    scenario.data,\n",
        "    explode_paths=scenario.explode_paths,\n",
        "    list_policy=scenario.list_policy,\n",
        ")\n",
        "\n",
        "csv_path = OUTPUT_DIR / \"items_pipeline.csv\"\n",
        "write_csv(records, csv_path)\n",
        "\n",
        "print(f\"‚úì Wrote {len(records)} records to {csv_path}\")\n",
        "print(\"\\nRound-trip read:\")\n",
        "round_trip = read_csv(csv_path)\n",
        "for row in round_trip:\n",
        "    print(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a id=\"milestone-7\"></a>\n",
        "\n",
        "# Milestone 7: MongoDB Integration\n",
        "\n",
        "## Learning Objectives\n",
        "- Ingest flattened records into MongoDB\n",
        "- Query collections for analytics\n",
        "- Understand type inference behavior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if MONGO_AVAILABLE:\n",
        "    mongo_uri = os.getenv(\"MONGO_URI\", \"mongodb://localhost:27017\")\n",
        "    database_name = os.getenv(\"MONGO_DB\", \"json_flatten_demo\")\n",
        "    collection_name = os.getenv(\"MONGO_COLLECTION\", \"orders\")\n",
        "\n",
        "    print_section(\"MongoDB ingestion\")\n",
        "    try:\n",
        "        inserted = ingest_csv_to_mongodb(\n",
        "            records,\n",
        "            mongo_uri=mongo_uri,\n",
        "            database_name=database_name,\n",
        "            collection_name=collection_name,\n",
        "        )\n",
        "        print(f\"Inserted {inserted} documents into {database_name}.{collection_name}\")\n",
        "\n",
        "        sample = query_mongodb(\n",
        "            mongo_uri=mongo_uri,\n",
        "            database_name=database_name,\n",
        "            collection_name=collection_name,\n",
        "            limit=3,\n",
        "        )\n",
        "        print(\"Sample documents:\")\n",
        "        for doc in sample:\n",
        "            print(doc)\n",
        "    except Exception as exc:\n",
        "        print(f\"MongoDB ingestion skipped: {exc}\")\n",
        "else:\n",
        "    print(\"MongoDB integration not available in this environment.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a id=\"milestone-8\"></a>\n",
        "\n",
        "# Milestone 8: Snowflake Integration\n",
        "\n",
        "## Learning Objectives\n",
        "- Generate Snowflake table schemas\n",
        "- Ingest CSV data into Snowflake\n",
        "- Query flattened data with SQL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if SNOWFLAKE_AVAILABLE:\n",
        "    print_section(\"Snowflake schema generation\")\n",
        "    try:\n",
        "        schema_sql = create_table_schema(records, \"orders_flat\", \"public\")\n",
        "        print(schema_sql)\n",
        "\n",
        "        # Optional: ingest if credentials are set in environment variables\n",
        "        required_env = [\n",
        "            \"SNOWFLAKE_ACCOUNT\",\n",
        "            \"SNOWFLAKE_USER\",\n",
        "            \"SNOWFLAKE_PASSWORD\",\n",
        "            \"SNOWFLAKE_WAREHOUSE\",\n",
        "            \"SNOWFLAKE_DATABASE\",\n",
        "            \"SNOWFLAKE_SCHEMA\",\n",
        "        ]\n",
        "        if all(os.getenv(key) for key in required_env):\n",
        "            count = ingest_csv_to_snowflake(\n",
        "                records,\n",
        "                account=os.getenv(\"SNOWFLAKE_ACCOUNT\"),\n",
        "                user=os.getenv(\"SNOWFLAKE_USER\"),\n",
        "                password=os.getenv(\"SNOWFLAKE_PASSWORD\"),\n",
        "                warehouse=os.getenv(\"SNOWFLAKE_WAREHOUSE\"),\n",
        "                database=os.getenv(\"SNOWFLAKE_DATABASE\"),\n",
        "                schema=os.getenv(\"SNOWFLAKE_SCHEMA\"),\n",
        "                table_name=\"orders_flat\",\n",
        "            )\n",
        "            print(f\"Ingested {count} rows into Snowflake\")\n",
        "        else:\n",
        "            print(\"Snowflake credentials not set; ingestion skipped.\")\n",
        "    except Exception as exc:\n",
        "        print(f\"Snowflake integration skipped: {exc}\")\n",
        "else:\n",
        "    print(\"Snowflake integration not available in this environment.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a id=\"milestone-9\"></a>\n",
        "\n",
        "# Milestone 9: Advanced Patterns & Best Practices\n",
        "\n",
        "## Learning Objectives\n",
        "- Measure performance for large workloads\n",
        "- Avoid unintended cartesian explosions\n",
        "- Leverage PySpark for scale when available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@measure_time\n",
        "def flatten_large_cartesian():\n",
        "    scenario = get_scenario_by_name(\"large_cartesian_product\")\n",
        "    return flatten_records(\n",
        "        scenario.data,\n",
        "        explode_paths=scenario.explode_paths,\n",
        "        list_policy=scenario.list_policy,\n",
        "    )\n",
        "\n",
        "large_records = flatten_large_cartesian()\n",
        "print(f\"Generated {len(large_records)} records from cartesian explosion\")\n",
        "\n",
        "print_section(\"PySpark (optional)\")\n",
        "if PYSPARK_AVAILABLE:\n",
        "    scenario = get_scenario_by_name(\"multi_path_explosion\")\n",
        "    df = spark.createDataFrame([scenario.data])\n",
        "    df.select(\"order_id\", explode(\"items\").alias(\"item\")) \\\n",
        "        .select(\"order_id\", col(\"item.sku\").alias(\"item_sku\")) \\\n",
        "        .show(truncate=False)\n",
        "else:\n",
        "    print(\"PySpark not available; skipping Spark example.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a id=\"milestone-10\"></a>\n",
        "\n",
        "# Milestone 10: End-to-End Workflows\n",
        "\n",
        "## Learning Objectives\n",
        "- Build a complete JSON ‚Üí CSV pipeline\n",
        "- Validate round-trip data integrity\n",
        "- Prepare data for database ingestion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_path = Path(\"data/sample.json\")\n",
        "sample = json.loads(sample_path.read_text())\n",
        "\n",
        "flattened = flatten_json(sample, list_policy=\"index\")\n",
        "end_to_end_path = OUTPUT_DIR / \"sample_flat.csv\"\n",
        "write_csv([flattened], end_to_end_path)\n",
        "\n",
        "print(f\"‚úì Wrote flattened sample to {end_to_end_path}\")\n",
        "print(\"Round-trip read:\")\n",
        "print(read_csv(end_to_end_path))\n",
        "\n",
        "if MONGO_AVAILABLE:\n",
        "    print(\"You can now ingest this CSV into MongoDB with ingest_csv_to_mongodb().\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Troubleshooting & Tips\n",
        "\n",
        "If a cell fails, try these first:\n",
        "- **Imports fail**: run the first setup cell again and verify you started Jupyter from the repo root.\n",
        "- **Module not found in Docker**: ensure the notebook was started with `make notebook-docker` and the repo is mounted.\n",
        "- **MongoDB errors**: confirm a local MongoDB is running and `MONGO_URI` points to it.\n",
        "- **Snowflake errors**: verify environment variables (`SNOWFLAKE_*`) and network access.\n",
        "- **PySpark errors**: confirm Java is installed and Docker has enough memory (4GB+).\n",
        "\n",
        "Tip: restart the kernel and re-run all cells if the environment feels inconsistent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Puzzle for Data Scientists\n",
        "\n",
        "You receive 1,000 JSON records with **three array fields**: `items` (avg 4), `promos` (avg 2), and `regions` (avg 3). You flatten by exploding all three paths to create a cartesian product.\n",
        "\n",
        "**Riddle:**\n",
        "- How many records do you expect on average after explosion?\n",
        "- If one region is missing (empty list) in 10% of records, how does that change the expected total?\n",
        "\n",
        "Write your answer and then validate by simulating a small sample in code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "num_records = 1000\n",
        "avg_items = 4\n",
        "avg_promos = 2\n",
        "avg_regions = 3\n",
        "missing_region_rate = 0.10\n",
        "\n",
        "# Quick Monte Carlo simulation for expected exploded record count\n",
        "simulated_total = 0\n",
        "for _ in range(num_records):\n",
        "    items = max(1, int(random.expovariate(1 / avg_items)))\n",
        "    promos = max(1, int(random.expovariate(1 / avg_promos)))\n",
        "\n",
        "    if random.random() < missing_region_rate:\n",
        "        regions = 0\n",
        "    else:\n",
        "        regions = max(1, int(random.expovariate(1 / avg_regions)))\n",
        "\n",
        "    simulated_total += items * promos * max(1, regions)\n",
        "\n",
        "print(f\"Simulated total records: {simulated_total}\")\n",
        "print(f\"Average per input record: {simulated_total / num_records:.2f}\")\n",
        "\n",
        "expected_no_missing = avg_items * avg_promos * avg_regions\n",
        "expected_with_missing = avg_items * avg_promos * ((1 - missing_region_rate) * avg_regions + missing_region_rate * 1)\n",
        "\n",
        "print(f\"Expected (no missing regions): {expected_no_missing:.2f}\")\n",
        "print(f\"Expected (10% missing regions): {expected_with_missing:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Puzzle 2: The Duplicate-Key Trap (Intermediate)\n",
        "\n",
        "You flatten 50,000 JSON docs. Each doc has an array `events` with a `type` field. You explode `events` and count `type` frequencies. Later you discover some events have duplicated keys (e.g., `\"type\"` appears twice in the raw JSON, last one wins during parsing).\n",
        "\n",
        "**Questions:**\n",
        "- How could this bias your frequency counts?\n",
        "- What visual signal would you expect if you plot top-10 type frequencies before vs after a \"last-key-wins\" parser?\n",
        "\n",
        "Validate by simulating a small dataset and plotting the before/after counts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "random.seed(7)\n",
        "\n",
        "# Simulate \"true\" types and an alternate value that overwrites it\n",
        "true_types = [\"click\", \"view\", \"purchase\", \"refund\", \"signup\"]\n",
        "shadow_types = [\"click\", \"view\", \"purchase\", \"fraud\", \"bot\"]\n",
        "\n",
        "n_events = 2000\n",
        "true_counts = Counter()\n",
        "parsed_counts = Counter()\n",
        "\n",
        "for _ in range(n_events):\n",
        "    t = random.choices(true_types, weights=[50, 30, 10, 5, 5])[0]\n",
        "    s = random.choices(shadow_types, weights=[40, 25, 15, 10, 10])[0]\n",
        "    # \"Raw\" event has duplicated key; parser keeps last value (s)\n",
        "    true_counts[t] += 1\n",
        "    parsed_counts[s] += 1\n",
        "\n",
        "labels = sorted(set(true_counts) | set(parsed_counts))\n",
        "true_vals = [true_counts[l] for l in labels]\n",
        "parsed_vals = [parsed_counts[l] for l in labels]\n",
        "\n",
        "x = range(len(labels))\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.bar(x, true_vals, alpha=0.7, label=\"Before (true)\")\n",
        "plt.bar(x, parsed_vals, alpha=0.7, label=\"After (parsed)\")\n",
        "plt.xticks(x, labels, rotation=30)\n",
        "plt.title(\"Type frequency shift from duplicate-key parsing\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Top-5 before:\", true_counts.most_common(5))\n",
        "print(\"Top-5 after:\", parsed_counts.most_common(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Puzzle 3: Missingness Meets Explosion (Intermediate+)\n",
        "\n",
        "You explode `items` (avg 5) and `coupons` (avg 2). But `coupons` is missing in 30% of records. You keep missing paths as `None` (not dropping records).\n",
        "\n",
        "**Questions:**\n",
        "- What is the expected multiplier on record count?\n",
        "- How does the distribution look compared to the no-missing case?\n",
        "\n",
        "Simulate and plot the distribution of expanded record counts per input record."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "random.seed(21)\n",
        "\n",
        "n = 5000\n",
        "avg_items = 5\n",
        "avg_coupons = 2\n",
        "missing_rate = 0.30\n",
        "\n",
        "counts_missing = []\n",
        "counts_full = []\n",
        "\n",
        "for _ in range(n):\n",
        "    items = max(1, int(random.expovariate(1 / avg_items)))\n",
        "    coupons = max(1, int(random.expovariate(1 / avg_coupons)))\n",
        "    coupons_missing = 1 if random.random() < missing_rate else coupons\n",
        "\n",
        "    counts_missing.append(items * coupons_missing)\n",
        "    counts_full.append(items * coupons)\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.hist(counts_full, bins=30, alpha=0.6, label=\"No missing coupons\")\n",
        "plt.hist(counts_missing, bins=30, alpha=0.6, label=\"30% missing (kept as None)\")\n",
        "plt.title(\"Explosion size per record\")\n",
        "plt.xlabel(\"Expanded records per input\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Expected no-missing multiplier:\", avg_items * avg_coupons)\n",
        "print(\"Expected with missing multiplier:\", avg_items * ((1 - missing_rate) * avg_coupons + missing_rate * 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Puzzle 4: Schema Drift Heatmap (Advanced)\n",
        "\n",
        "You flatten daily event logs and track the **set of flattened keys** per day. Over a month, product teams add and remove fields.\n",
        "\n",
        "**Questions:**\n",
        "- How would you visualize drift so that it highlights new, removed, and rare fields?\n",
        "- Which day shows the most schema churn?\n",
        "\n",
        "Simulate 30 days of keys and plot a heatmap of key presence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "random.seed(5)\n",
        "np.random.seed(5)\n",
        "\n",
        "base_keys = [f\"k{i}\" for i in range(1, 21)]\n",
        "extra_keys = [f\"x{i}\" for i in range(1, 16)]\n",
        "\n",
        "days = 30\n",
        "all_keys = base_keys + extra_keys\n",
        "presence = []\n",
        "\n",
        "for day in range(days):\n",
        "    day_keys = set(base_keys)\n",
        "    # Gradually introduce extra keys\n",
        "    for key in extra_keys:\n",
        "        if random.random() < (day / days):\n",
        "            day_keys.add(key)\n",
        "    # Random removals\n",
        "    for key in list(day_keys):\n",
        "        if random.random() < 0.05:\n",
        "            day_keys.remove(key)\n",
        "    presence.append([1 if key in day_keys else 0 for key in all_keys])\n",
        "\n",
        "presence = np.array(presence)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(presence, aspect=\"auto\", cmap=\"viridis\")\n",
        "plt.colorbar(label=\"Key present\")\n",
        "plt.yticks(range(0, days, 5), [f\"Day {i+1}\" for i in range(0, days, 5)])\n",
        "plt.xticks(range(0, len(all_keys), 5), all_keys[::5], rotation=45)\n",
        "plt.title(\"Schema drift heatmap\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "churn = np.abs(np.diff(presence, axis=0)).sum(axis=1)\n",
        "max_day = int(np.argmax(churn)) + 2\n",
        "print(f\"Highest churn on Day {max_day}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Puzzle 5: Simpson‚Äôs Paradox in Flattened Metrics (Advanced)\n",
        "\n",
        "You flatten experiments by user and compute conversion rate by `device`. Overall, **mobile** has higher conversion. But after segmenting by `region`, desktop wins in every region.\n",
        "\n",
        "**Questions:**\n",
        "- How can this happen?\n",
        "- What would the plot of per-region conversion rates look like vs the overall rate?\n",
        "\n",
        "Simulate and plot the overall vs per-region conversion rates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "random.seed(11)\n",
        "\n",
        "regions = [\"NA\", \"EU\", \"APAC\"]\n",
        "# Desktop better within each region, but mobile more common in high-converting regions\n",
        "region_sizes = {\"NA\": 4000, \"EU\": 3000, \"APAC\": 1000}\n",
        "conversion = {\n",
        "    \"NA\": {\"desktop\": 0.06, \"mobile\": 0.05},\n",
        "    \"EU\": {\"desktop\": 0.08, \"mobile\": 0.07},\n",
        "    \"APAC\": {\"desktop\": 0.04, \"mobile\": 0.03},\n",
        "}\n",
        "# Device mix skews mobile to NA/EU where rates are higher\n",
        "device_mix = {\n",
        "    \"NA\": {\"desktop\": 0.30, \"mobile\": 0.70},\n",
        "    \"EU\": {\"desktop\": 0.25, \"mobile\": 0.75},\n",
        "    \"APAC\": {\"desktop\": 0.70, \"mobile\": 0.30},\n",
        "}\n",
        "\n",
        "counts = {r: {\"desktop\": [0, 0], \"mobile\": [0, 0]} for r in regions}\n",
        "\n",
        "for region in regions:\n",
        "    for _ in range(region_sizes[region]):\n",
        "        device = \"mobile\" if random.random() < device_mix[region][\"mobile\"] else \"desktop\"\n",
        "        conv = 1 if random.random() < conversion[region][device] else 0\n",
        "        counts[region][device][0] += conv\n",
        "        counts[region][device][1] += 1\n",
        "\n",
        "# Overall rates\n",
        "overall = {\"desktop\": [0, 0], \"mobile\": [0, 0]}\n",
        "for region in regions:\n",
        "    for device in [\"desktop\", \"mobile\"]:\n",
        "        overall[device][0] += counts[region][device][0]\n",
        "        overall[device][1] += counts[region][device][1]\n",
        "\n",
        "rate_overall = {\n",
        "    d: overall[d][0] / overall[d][1] for d in overall\n",
        "}\n",
        "\n",
        "# Plot per-region vs overall\n",
        "plt.figure(figsize=(8, 4))\n",
        "for device in [\"desktop\", \"mobile\"]:\n",
        "    regional_rates = [counts[r][device][0] / counts[r][device][1] for r in regions]\n",
        "    plt.plot(regions, regional_rates, marker=\"o\", label=f\"{device} (by region)\")\n",
        "\n",
        "plt.hlines(rate_overall[\"desktop\"], 0, len(regions) - 1, colors=\"C0\", linestyles=\"--\", label=\"desktop overall\")\n",
        "plt.hlines(rate_overall[\"mobile\"], 0, len(regions) - 1, colors=\"C1\", linestyles=\"--\", label=\"mobile overall\")\n",
        "plt.title(\"Simpson's paradox: overall vs per-region\")\n",
        "plt.ylabel(\"Conversion rate\")\n",
        "plt.xticks(range(len(regions)), regions)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Overall rates:\", rate_overall)\n",
        "for region in regions:\n",
        "    print(region, {d: counts[region][d][0] / counts[region][d][1] for d in [\"desktop\", \"mobile\"]})"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
