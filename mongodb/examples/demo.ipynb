{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# JSON Flattening Toolkit - Comprehensive Guide for Data Engineers & Scientists\n",
        "\n",
        "> **A world-class exploration of JSON flattening techniques, patterns, and real-world applications**\n",
        "\n",
        "This notebook is organized into **10 self-contained milestones**, each focusing on specific aspects of JSON flattening. You can work through them sequentially or jump to specific topics of interest.\n",
        "\n",
        "## üìö Table of Contents\n",
        "\n",
        "### Foundations\n",
        "- **[Milestone 1: Foundations & Core Concepts](#milestone-1)** - Basic flattening, list policies, separators\n",
        "- **[Milestone 2: Array Handling Strategies](#milestone-2)** - Index vs join, explosion, cartesian products\n",
        "\n",
        "### Advanced Techniques  \n",
        "- **[Milestone 3: Complex Structures](#milestone-3)** - Deep nesting, mixed types, null handling\n",
        "\n",
        "### Real-World Use Cases\n",
        "- **[Milestone 4: E-commerce Data](#milestone-4)** - Orders, products, customers, transactions\n",
        "- **[Milestone 5: API & Event Data](#milestone-5)** - API responses, webhooks, event logs\n",
        "\n",
        "### Data Pipelines\n",
        "- **[Milestone 6: CSV Operations & Pipelines](#milestone-6)** - Read/write, transformations, batch processing\n",
        "\n",
        "### Database Integration\n",
        "- **[Milestone 7: MongoDB Integration](#milestone-7)** - Ingestion, querying, type inference\n",
        "- **[Milestone 8: Snowflake Integration](#milestone-8)** - Schema generation, ingestion, queries\n",
        "\n",
        "### Production Patterns\n",
        "- **[Milestone 9: Advanced Patterns & Best Practices](#milestone-9)** - Performance, memory, error handling\n",
        "- **[Milestone 10: End-to-End Workflows](#milestone-10)** - Complete pipelines, production examples\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will be able to:\n",
        "- ‚úÖ Flatten complex nested JSON structures efficiently\n",
        "- ‚úÖ Choose appropriate array handling strategies for your use case\n",
        "- ‚úÖ Build data pipelines from JSON to CSV to databases\n",
        "- ‚úÖ Handle edge cases (nulls, empty arrays, mixed types)\n",
        "- ‚úÖ Integrate with MongoDB and Snowflake\n",
        "- ‚úÖ Apply best practices for production systems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Quick Start\n",
        "\n",
        "Let's set up our environment and import the necessary modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# IMPORTS - All imports at the top for clarity\n",
        "# ============================================================================\n",
        "\n",
        "import json\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import Any, Dict, List, Optional\n",
        "from collections import Counter\n",
        "\n",
        "# Core flattening functions\n",
        "from json_flatten import flatten_json, flatten_records, write_csv, read_csv\n",
        "from json_flatten.scenarios import get_scenarios\n",
        "\n",
        "# Optional: MongoDB and Snowflake (may not be available)\n",
        "try:\n",
        "    from json_flatten.mongodb_io import ingest_csv_to_mongodb, query_mongodb, infer_type\n",
        "    MONGO_AVAILABLE = True\n",
        "except ImportError:\n",
        "    MONGO_AVAILABLE = False\n",
        "    print(\"‚ö† MongoDB integration not available (pymongo not installed)\")\n",
        "\n",
        "try:\n",
        "    from json_flatten.snowflake_io import create_table_schema, ingest_csv_to_snowflake, query_snowflake\n",
        "    SNOWFLAKE_AVAILABLE = True\n",
        "except ImportError:\n",
        "    SNOWFLAKE_AVAILABLE = False\n",
        "    print(\"‚ö† Snowflake integration not available (snowflake-connector-python not installed)\")\n",
        "\n",
        "# PySpark imports (for large-scale processing)\n",
        "try:\n",
        "    import findspark\n",
        "    findspark.init()\n",
        "    from pyspark.sql import SparkSession\n",
        "    from pyspark.sql.functions import col, explode, from_json, schema_of_json\n",
        "    from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, BooleanType, ArrayType\n",
        "    PYSPARK_AVAILABLE = True\n",
        "except ImportError:\n",
        "    PYSPARK_AVAILABLE = False\n",
        "    print(\"‚ö† PySpark not available (pyspark not installed)\")\n",
        "\n",
        "# Setup output directory\n",
        "OUTPUT_DIR = Path(\"notebook_output\")\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Helper functions for pretty printing and analysis\n",
        "def print_section(title: str, char: str = \"=\"):\n",
        "    \"\"\"Print a formatted section header.\"\"\"\n",
        "    print(f\"\\n{char * 60}\")\n",
        "    print(f\"  {title}\")\n",
        "    print(f\"{char * 60}\\n\")\n",
        "\n",
        "def compare_before_after(before: Any, after: Dict[str, Any], title: str = \"Transformation\"):\n",
        "    \"\"\"Compare original and flattened data side by side.\"\"\"\n",
        "    print_section(title)\n",
        "    print(\"BEFORE (Original JSON):\")\n",
        "    print(json.dumps(before, indent=2))\n",
        "    print(\"\\nAFTER (Flattened):\")\n",
        "    print(json.dumps(after, indent=2))\n",
        "    print(f\"\\nüìä Flattened to {len(after)} fields\")\n",
        "\n",
        "def measure_time(func):\n",
        "    \"\"\"Decorator to measure execution time.\"\"\"\n",
        "    def wrapper(*args, **kwargs):\n",
        "        start = time.time()\n",
        "        result = func(*args, **kwargs)\n",
        "        elapsed = time.time() - start\n",
        "        print(f\"‚è±Ô∏è  Execution time: {elapsed:.4f} seconds\")\n",
        "        return result\n",
        "    return wrapper\n",
        "\n",
        "# Initialize PySpark if available\n",
        "if PYSPARK_AVAILABLE:\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"JSONFlattening\") \\\n",
        "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "        .getOrCreate()\n",
        "    spark.sparkContext.setLogLevel(\"WARN\")  # Reduce verbosity\n",
        "    print(\"‚úÖ PySpark session initialized\")\n",
        "\n",
        "print(\"‚úÖ Environment setup complete!\")\n",
        "print(f\"üìÅ Output directory: {OUTPUT_DIR.absolute()}\")\n",
        "print(f\"üîß MongoDB available: {MONGO_AVAILABLE}\")\n",
        "print(f\"‚ùÑÔ∏è  Snowflake available: {SNOWFLAKE_AVAILABLE}\")\n",
        "print(f\"‚ö° PySpark available: {PYSPARK_AVAILABLE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Milestone 1: Foundations & Core Concepts\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand the fundamental concept of JSON flattening\n",
        "- Learn how nested structures are converted to flat dictionaries\n",
        "- Explore different list handling policies\n",
        "- Master custom separator usage\n",
        "\n",
        "## Why Flatten JSON?\n",
        "\n",
        "Data engineers and data scientists frequently encounter challenges:\n",
        "- **Tabular formats** (CSV, databases) require flat structures\n",
        "- **Analytics tools** work better with normalized data\n",
        "- **Schema inference** is easier with flat structures\n",
        "- **Database ingestion** requires consistent column structures\n",
        "\n",
        "Let's start with the basics!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### 1.1 Understanding Nested Structures\n",
        "\n",
        "**What is nesting?**  \n",
        "Nesting occurs when JSON objects contain other objects or arrays inside them. Think of it like Russian dolls - objects within objects.\n",
        "\n",
        "**Why is this a problem?**  \n",
        "- Databases expect flat tables with columns\n",
        "- CSV files are inherently flat (rows and columns)\n",
        "- Analytics tools work better with normalized data\n",
        "- Schema inference becomes complex with nested structures\n",
        "\n",
        "**How does flattening work?**  \n",
        "The `flatten_json()` function recursively traverses nested structures and creates dot-delimited keys. For example:\n",
        "- `user.profile.name` represents the `name` field inside `profile` inside `user`\n",
        "- The dot (`.`) is the default separator, but you can customize it\n",
        "\n",
        "Let's see this in action:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 1: Simple nested structure\n",
        "data1 = {\n",
        "    \"user\": {\n",
        "        \"id\": 42,\n",
        "        \"profile\": {\n",
        "            \"name\": \"Alice\",\n",
        "            \"active\": True\n",
        "        }\n",
        "    },\n",
        "    \"score\": 9.5\n",
        "}\n",
        "\n",
        "flattened1 = flatten_json(data1)\n",
        "compare_before_after(data1, flattened1, \"Example 1: Simple Nested Structure\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Custom Separators\n",
        "\n",
        "**Why use custom separators?**  \n",
        "Sometimes the default dot (`.`) separator can conflict with your data:\n",
        "- Field names might contain dots\n",
        "- You might prefer underscores (`_`) or double underscores (`__`)\n",
        "- Some systems have naming conventions\n",
        "\n",
        "**Example use cases:**\n",
        "- MongoDB uses dots for nested queries, so you might want `_` instead\n",
        "- Some databases prefer `__` for clarity\n",
        "- Your organization might have specific naming standards\n",
        "\n",
        "Let's explore different separators:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Array Handling\n",
        "\n",
        "Arrays can be handled in two ways:\n",
        "- **Index policy**: Creates indexed keys (e.g., `tags.0`, `tags.1`)\n",
        "- **Join policy**: Joins primitive arrays with commas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Array explosion - creating multiple records\n",
        "data5 = {\n",
        "    \"order_id\": 1001,\n",
        "    \"customer\": \"Alice\",\n",
        "    \"items\": [\n",
        "        {\"sku\": \"A1\", \"qty\": 2, \"price\": 10.50},\n",
        "        {\"sku\": \"B2\", \"qty\": 1, \"price\": 5.25},\n",
        "        {\"sku\": \"C3\", \"qty\": 3, \"price\": 8.00}\n",
        "    ]\n",
        "}\n",
        "\n",
        "records = flatten_records(data5, explode_paths=[\"items\"])\n",
        "print(f\"Created {len(records)} records from array explosion:\")\n",
        "for i, record in enumerate(records, 1):\n",
        "    print(f\"\\nRecord {i}:\")\n",
        "    print(json.dumps(record, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CSV Operations\n",
        "\n",
        "Converting flattened JSON to CSV format for database ingestion or analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create output directory\n",
        "output_dir = Path(\"notebook_output\")\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Flatten and write to CSV\n",
        "sample_data = {\n",
        "    \"order_id\": 1001,\n",
        "    \"customer\": {\"name\": \"Alice\", \"email\": \"alice@example.com\"},\n",
        "    \"items\": [\n",
        "        {\"sku\": \"A1\", \"qty\": 2},\n",
        "        {\"sku\": \"B2\", \"qty\": 1}\n",
        "    ]\n",
        "}\n",
        "\n",
        "records = flatten_records(sample_data, explode_paths=[\"items\"])\n",
        "csv_path = output_dir / \"orders.csv\"\n",
        "write_csv(records, csv_path)\n",
        "\n",
        "print(f\"‚úì Written {len(records)} records to {csv_path}\")\n",
        "print(\"\\nCSV content:\")\n",
        "print(csv_path.read_text())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This toolkit provides comprehensive solutions for:\n",
        "1. **Flattening complex JSON structures** with configurable policies\n",
        "2. **Handling arrays** through indexing or explosion\n",
        "3. **Creating cartesian products** from multiple array paths\n",
        "4. **CSV conversion** for tabular data formats\n",
        "5. **Database ingestion** into MongoDB and Snowflake\n",
        "\n",
        "See README.md for complete documentation."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
